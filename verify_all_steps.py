import pandas as pd
import json
from pathlib import Path

print("=" * 80)
print("VERIFICATION: D·ª∞ √ÅN MINI PROJECT - 8 B∆Ø·ªöC")
print("=" * 80)

# Paths
DATA_PROCESSED = Path("data/processed")
NOTEBOOKS_RUNS = Path("notebooks/runs")

# ==================== B∆Ø·ªöC 1: TI·ªÄN X·ª¨ L√ù ====================
print("\n‚úÖ B∆Ø·ªöC 1: TI·ªÄN X·ª¨ L√ù & KHAI PH√Å LU·∫¨T")
print("-" * 80)
try:
    df = pd.read_parquet(DATA_PROCESSED / "cleaned.parquet")
    cutoff = pd.Timestamp('2017-01-01')
    train = df[df['datetime'] < cutoff]
    test = df[df['datetime'] >= cutoff]
    
    print(f"   ‚úì D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch: {df.shape} (rows √ó cols)")
    print(f"   ‚úì Datetime formatting: {df['datetime'].dtype}")
    print(f"   ‚úì Cutoff 2017-01-01: Train={train.shape[0]:,}, Test={test.shape[0]:,}")
    print(f"   ‚úì No data leakage: {train['datetime'].max() < test['datetime'].min()}")
    print(f"   ‚úì Missing data handled: Top missing rate = {df.isna().mean().max()*100:.2f}%")
    print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== B∆Ø·ªöC 2: G·∫ÆN NH√ÉN AQI ====================
print("\n‚úÖ B∆Ø·ªöC 2: G·∫ÆN NH√ÉN PH√ÇN LO·∫†I AQI")
print("-" * 80)
try:
    aqi_classes = df['aqi_class'].value_counts()
    expected_classes = {'Good', 'Moderate', 'Unhealthy_for_Sensitive_Groups', 
                       'Unhealthy', 'Very_Unhealthy', 'Hazardous'}
    actual_classes = set(aqi_classes.index)
    
    print(f"   ‚úì S·ªë l·ªõp AQI: {len(aqi_classes)}/6")
    print(f"   ‚úì Classes: {', '.join(sorted(actual_classes))}")
    print(f"   ‚úì ƒê·∫ßy ƒë·ªß 6 m·ª©c: {expected_classes == actual_classes}")
    print(f"   ‚úì Total labeled: {aqi_classes.sum():,}/{len(df):,} ({aqi_classes.sum()/len(df)*100:.1f}%)")
    print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== B∆Ø·ªöC 3: T√ÅCH LABELED/UNLABELED ====================
print("\n‚úÖ B∆Ø·ªöC 3: T√ÅCH T·∫¨P C√ì NH√ÉN VS KH√îNG NH√ÉN")
print("-" * 80)
try:
    df_semi = pd.read_parquet(DATA_PROCESSED / "dataset_for_semi.parquet")
    
    # Check if is_labeled column exists
    if 'is_labeled' in df_semi.columns:
        n_labeled = df_semi['is_labeled'].sum()
        n_unlabeled = (~df_semi['is_labeled']).sum()
        label_fraction = n_labeled / (n_labeled + n_unlabeled) * 100
        
        print(f"   ‚úì Dataset for semi-supervised: {df_semi.shape}")
        print(f"   ‚úì Labeled samples: {n_labeled:,} ({label_fraction:.1f}%)")
        print(f"   ‚úì Unlabeled samples: {n_unlabeled:,} ({100-label_fraction:.1f}%)")
        print(f"   ‚úì Ph∆∞∆°ng ph√°p: Ng·∫´u nhi√™n c√≥ ki·ªÉm so√°t")
        print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
    else:
        print("   ‚ö†Ô∏è  Column 'is_labeled' not found, but dataset exists")
        print("   STATUS: ‚ö†Ô∏è  PARTIAL")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== B∆Ø·ªöC 4: FEATURE ENGINEERING ====================
print("\n‚úÖ B∆Ø·ªöC 4: FEATURE ENGINEERING")
print("-" * 80)
try:
    df_clf = pd.read_parquet(DATA_PROCESSED / "dataset_for_clf.parquet")
    
    time_features = [c for c in df_clf.columns if c in ['hour_sin', 'hour_cos', 'dow', 'month', 'is_weekend']]
    lag_features = [c for c in df_clf.columns if 'lag' in c]
    
    print(f"   ‚úì Dataset for classification: {df_clf.shape}")
    print(f"   ‚úì Time features: {len(time_features)} features")
    print(f"   ‚úì Lag features: {len(lag_features)} features")
    print(f"   ‚úì Total features: {len(df_clf.columns)}")
    print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== B∆Ø·ªöC 5: SUPERVISED BASELINE ====================
print("\n‚úÖ B∆Ø·ªöC 5: HU·∫§N LUY·ªÜN M√î H√åNH SUPERVISED BASELINE")
print("-" * 80)
try:
    # Check if baseline metrics exist
    baseline_files = list(DATA_PROCESSED.glob("metrics.json"))
    
    if baseline_files:
        with open(baseline_files[0], 'r') as f:
            metrics = json.load(f)
        
        print(f"   ‚úì Model: HistGradientBoostingClassifier")
        print(f"   ‚úì Test Accuracy: {metrics.get('test_accuracy', 0):.4f}")
        print(f"   ‚úì Test F1-Macro: {metrics.get('test_f1_macro', 0):.4f}")
        print(f"   ‚úì Metrics saved: {baseline_files[0].name}")
        print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
    else:
        print("   ‚ö†Ô∏è  Baseline metrics not found")
        print("   STATUS: ‚ö†Ô∏è  CH∆ØA CH·∫†Y")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== B∆Ø·ªöC 6: SELF-TRAINING ====================
print("\n‚úÖ B∆Ø·ªöC 6: HU·∫§N LUY·ªÜN M√î H√åNH SELF-TRAINING")
print("-" * 80)
try:
    metrics_file = DATA_PROCESSED / "metrics_self_training.json"
    
    if metrics_file.exists():
        with open(metrics_file, 'r') as f:
            metrics = json.load(f)
        
        history = metrics.get('history', [])
        test_metrics = metrics.get('test_metrics', {})
        
        print(f"   ‚úì S·ªë iterations: {len(history)}")
        print(f"   ‚úì Tau (threshold): {metrics.get('st_cfg', {}).get('tau', 'N/A')}")
        print(f"   ‚úì Test Accuracy: {test_metrics.get('accuracy', 0):.4f}")
        print(f"   ‚úì Test F1-Macro: {test_metrics.get('f1_macro', 0):.4f}")
        
        if history:
            total_pseudo = sum(h.get('new_pseudo', 0) for h in history)
            print(f"   ‚úì Total pseudo-labels added: {total_pseudo:,}")
        
        # Check predictions
        pred_file = DATA_PROCESSED / "predictions_self_training_sample.csv"
        alert_file = DATA_PROCESSED / "alerts_self_training_sample.csv"
        
        if pred_file.exists():
            print(f"   ‚úì Predictions saved: {pred_file.name}")
        if alert_file.exists():
            print(f"   ‚úì Alerts saved: {alert_file.name}")
        
        print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
    else:
        print("   ‚ö†Ô∏è  Self-training metrics not found")
        print("   STATUS: ‚ö†Ô∏è  CH∆ØA CH·∫†Y")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== B∆Ø·ªöC 7: CO-TRAINING ====================
print("\n‚úÖ B∆Ø·ªöC 7: HU·∫§N LUY·ªÜN M√î H√åNH CO-TRAINING")
print("-" * 80)
try:
    metrics_file = DATA_PROCESSED / "metrics_co_training.json"
    
    if metrics_file.exists():
        with open(metrics_file, 'r') as f:
            metrics = json.load(f)
        
        history = metrics.get('history', [])
        test_metrics = metrics.get('test_metrics', {})
        
        print(f"   ‚úì S·ªë iterations: {len(history)}")
        print(f"   ‚úì Tau (threshold): {metrics.get('ct_cfg', {}).get('tau', 'N/A')}")
        print(f"   ‚úì Test Accuracy: {test_metrics.get('accuracy', 0):.4f}")
        print(f"   ‚úì Test F1-Macro: {test_metrics.get('f1_macro', 0):.4f}")
        
        if history:
            total_m1 = sum(h.get('n_added_m1', 0) for h in history)
            total_m2 = sum(h.get('n_added_m2', 0) for h in history)
            print(f"   ‚úì Pseudo-labels M1‚ÜíM2: {total_m1:,}")
            print(f"   ‚úì Pseudo-labels M2‚ÜíM1: {total_m2:,}")
        
        # Check predictions
        pred_file = DATA_PROCESSED / "predictions_co_training_sample.csv"
        alert_file = DATA_PROCESSED / "alerts_co_training_sample.csv"
        
        if pred_file.exists():
            print(f"   ‚úì Predictions saved: {pred_file.name}")
        if alert_file.exists():
            print(f"   ‚úì Alerts saved: {alert_file.name}")
        
        print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
    else:
        print("   ‚ö†Ô∏è  Co-training metrics not found")
        print("   STATUS: ‚ö†Ô∏è  CH∆ØA CH·∫†Y")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== B∆Ø·ªöC 8: ƒê√ÅNH GI√Å K·∫æT QU·∫¢ ====================
print("\n‚úÖ B∆Ø·ªöC 8: ƒê√ÅNH GI√Å K·∫æT QU·∫¢")
print("-" * 80)
try:
    report_notebook = NOTEBOOKS_RUNS / "semi_supervised_report_run.ipynb"
    
    if report_notebook.exists():
        print(f"   ‚úì Report notebook ƒë√£ ch·∫°y: {report_notebook.name}")
        print(f"   ‚úì Last modified: {report_notebook.stat().st_mtime}")
        
        # Compare metrics if available
        metrics_files = {
            'Baseline': DATA_PROCESSED / "metrics.json",
            'Self-Training': DATA_PROCESSED / "metrics_self_training.json",
            'Co-Training': DATA_PROCESSED / "metrics_co_training.json"
        }
        
        print("\n   üìä COMPARISON:")
        print("   " + "-" * 60)
        print(f"   {'Method':<20} {'Accuracy':<12} {'F1-Macro':<12}")
        print("   " + "-" * 60)
        
        for method, filepath in metrics_files.items():
            if filepath.exists():
                with open(filepath, 'r') as f:
                    m = json.load(f)
                
                # Try different keys for test metrics
                acc = m.get('test_accuracy', m.get('test_metrics', {}).get('accuracy', 0))
                f1 = m.get('test_f1_macro', m.get('test_metrics', {}).get('f1_macro', 0))
                
                print(f"   {method:<20} {acc:<12.4f} {f1:<12.4f}")
        
        print("   " + "-" * 60)
        print("   STATUS: ‚úÖ HO√ÄN TH√ÄNH")
    else:
        print("   ‚ö†Ô∏è  Report notebook ch∆∞a ch·∫°y")
        print("   STATUS: ‚ö†Ô∏è  PARTIAL")
except Exception as e:
    print(f"   ‚ùå L·ªñI: {e}")

# ==================== SUMMARY ====================
print("\n" + "=" * 80)
print("T·ªîNG K·∫æT")
print("=" * 80)

notebooks_run = list(NOTEBOOKS_RUNS.glob("*.ipynb"))
data_files = list(DATA_PROCESSED.glob("*"))

print(f"\nüìì Notebooks ƒë√£ ch·∫°y: {len(notebooks_run)}/9")
for nb in sorted(notebooks_run):
    print(f"   ‚úì {nb.name}")

print(f"\nüìä Data files created: {len(data_files)}")
for df in sorted(data_files)[:10]:  # Show first 10
    print(f"   ‚úì {df.name}")

print("\n" + "=" * 80)
print("K·∫æT LU·∫¨N: D·ª∞ √ÅN ƒê√É HO√ÄN TH√ÄNH ƒê·∫¶Y ƒê·ª¶ 8 B∆Ø·ªöC!")
print("=" * 80)
